# LLM Service — чат, RAG и субагенты

## Назначение
LLM Service обрабатывает пользовательские запросы, собирает контекст из внутренних источников, маршрутизирует задачи субагентам и формирует ответы с цитированием релевантных материалов. Сервис ведет историю диалогов, управляет памятью организации и интегрируется с внешними системами.

## Основные возможности

### 1. Агентская архитектура
- **Главный агент (main)** - координирует работу и может делегировать задачи специализированным агентам
- **Субагенты**:
  - `marketing_agent` - эксперт по маркетингу и продвижению
  - `legal_agent` - правовой консультант
  - `business_analyst_agent` - бизнес-аналитик и эксперт по CRM
- **Динамическое переключение** между агентами в рамках одного диалога
- **Изоляция контекста** - каждый субагент работает в отдельном чате с возвратом результата

### 2. Система инструментов (Tools)
Агенты используют инструменты для выполнения конкретных действий:

#### Системные инструменты
- `switch_to_subagent` - переключение на специализированного агента
- `finish_subagent` - завершение работы субагента и возврат результата

#### Инструменты поиска
- `web_search` - веб-поиск через Tavily API для получения актуальной информации

#### Инструменты памяти
- `save_organization_note` - сохранение важных фактов об организации для использования в будущих диалогах

#### MCP инструменты (Model Context Protocol)
- Динамическая интеграция с внешними системами через MCP
- **AmoCRM инструменты** (префикс `ammo-crm-*`):
  - `ammo-crm-entity_get` - получение сущностей CRM
  - `ammo-crm-leads_pipelines_get` - получение воронок продаж
  - `ammo-crm-users_get` - получение пользователей CRM
  - И другие инструменты для работы с CRM

### 3. RAG (Retrieval-Augmented Generation)
- **Векторный поиск** релевантных фрагментов из документов организации
- **Автоматическое обогащение контекста** - подбор релевантных документов для ответа
- Интеграция с `docs-processor` для получения индексированных фрагментов
- **Цитирование источников** в ответах

### 4. Управление диалогами
- **Создание и хранение чатов** с привязкой к организации и пользователю
- **История сообщений** с поддержкой ролей (user, assistant, system, tool)
- **Вложенные чаты** для субагентов с возможностью возврата к родительскому контексту
- **Потоковая передача** ответов (streaming) через gRPC и WebSocket

### 5. Память организации
- **Долгосрочное хранение фактов** об организации (до 500 символов каждый)
- **Автоматическое включение** релевантных фактов в контекст диалога
- **CRUD операции** для управления фактами
- Уникальность фактов в рамках организации

### 6. Квотирование и лимиты
- **Дневные лимиты** на использование токенов LLM
- **Резервирование токенов** перед выполнением запроса
- **Подтверждение использования** с учетом фактического расхода
- **Отслеживание статистики** использования API

### 7. Интеграции

#### Document Processor Service
- Поиск релевантных фрагментов документов для RAG
- Получение контекста из индексированных документов

#### Внешние LLM провайдеры
- OpenAI API (основной провайдер)
- Поддержка streaming ответов
- Настройка моделей и параметров (reasoning effort)

#### Веб-поиск (Tavily API)
- Получение актуальной информации из интернета
- Настройка глубины поиска и количества результатов

#### MCP серверы
- Динамическое подключение к внешним системам
- Кеширование списка доступных инструментов
- SSE транспорт для коммуникации

## Потоки данных

### Обычный запрос в чат
1. Клиент отправляет сообщение через WebSocket/gRPC
2. Создается/обновляется чат
3. Сохраняется сообщение пользователя
4. Подбирается контекст:
   - Факты об организации из памяти
   - Релевантные фрагменты документов (RAG)
5. Формируется запрос к LLM с инструментами
6. LLM генерирует ответ (может вызывать инструменты)
7. Выполняются вызовы инструментов (если требуется)
8. Ответ стримится клиенту в real-time
9. Сохраняется итоговое сообщение ассистента

### Работа с субагентами
1. Главный агент вызывает `switch_to_subagent`
2. Создается дочерний чат для субагента
3. Субагент получает задачу и контекст
4. Субагент работает с задачей (может использовать свои инструменты)
5. Субагент вызывает `finish_subagent` с summary
6. Результат возвращается главному агенту
7. Дочерний чат переводится в статус `completed`

### Вызов инструментов
1. LLM решает использовать инструмент
2. Создается `tool_call` со статусом `pending`
3. Сервис выполняет инструмент:
   - Веб-поиск через Tavily
   - Сохранение факта в память
   - Вызов MCP инструмента
4. Результат сохраняется в `tool_call`
5. Результат отправляется обратно в LLM
6. LLM использует результат для формирования ответа

## Выбор моделей

### LLM модель: Kimi K2 Thinking
Для генерации ответов используется модель **kimi-k2-thinking**, которая обеспечивает:
- **Естественность диалога** - приятное общение с пользователем без излишней формальности
- **Высокая скорость** генерации ответов для комфортного real-time взаимодействия
- **Надежная работа с инструментами** (tool calling) - точное определение необходимых действий и корректное формирование параметров
- **Аналитические способности** - понимание сложных бизнес-запросов и контекста
- **Открытые веса** - возможность самостоятельного хостинга и кастомизации при необходимости

### Модель эмбеддингов: Qwen3 Embedding 0.6B
Для векторного поиска и RAG применяется **qwen/qwen3-embedding-0.6b**:
- **Компактный размер** (0.6B параметров) - быстрая генерация векторов при минимальных ресурсах
- **Мультиязычность** - качественная работа с русским и английским языками
- **Оптимизация для поиска** - высокая точность при поиске семантически близких документов
- **Низкая латентность** - мгновенное вычисление эмбеддингов даже при больших объемах документов
- **Энергоэффективность** - возможность работы на CPU без потери производительности

## Требования к доступу и контексту
- Запросы обрабатываются строго в контексте **организации пользователя**
- **JWT токен** содержит `user_id` и `organization_id`
- Ролевая модель учитывается при модификации данных
- **Изоляция данных** между организациями на уровне БД
